# Epoll

## 并发和并行

单核CPU能实现并行吗？不行。

单线程能实现高并发吗？可以。

并发和并行的区别是？一个看的是**时间段内**的执行情况，一个看的是时间**时刻**的执行情况。

单线程如何做到高并发？多路复用。

## IO多路复用

多路是指多个业务方（句柄）并发下来的IO；复用是指复用这一个后台处理程序。

业务方要创建多个`fd`，那么你就需要负责这些`fd`的处理，并且最好还要并发起来，让每一个`fd`通道有独占的错觉。

要求什么呢？快！快！快！这就是最核心的要求。

那我一个IO请求（比如`write`）对应一个线程来处理，这样所有的IO不都并发了吗？是可以，但是有瓶颈，线程数一旦多了，性能是反倒会差的。

> Note：这里不再对比多线程和IO多路复用实现高并发之间的区别，读者可以自行了解。

## 最朴实的实现方式？

我不用任何其他系统调用，能否实现IO多路复用？

可以的。那么写个`for`循环，每次都尝试IO一下，读/写到了就处理，读/写不到就`sleep`下。这样我们不就实现了1对多的IO多路复用嘛。

```C
while True:
    for each 句柄数组 {
        read/write(fd, /* 参数 */)
    }
    sleep(1s)
```

>Note：有个问题，上面的程序可能会被卡死在第三行，使得整个系统不得运行，为什么？
>
>默认情况下，我们没有加任何参数`create`出的句柄是阻塞类型的。我们读数据的时候，如果数据还没准备好，是会需要等待的，当我们写数据的时候，如果还没准备好，默认也会卡住等待。所以，在上面伪代码第三行是可能被直接卡死，而导致整个线程都得到不到运行。
>
>那这个问题怎么解决？只需要把fd都设置成非阻塞模式。
>

以上就是最朴实的IO多路复用的实现了。但是好像在生产环境没见过这种IO多路复用的实现？为什么？

因为还不够高级。`for`循环每次要定期`sleep 1s`，这个会导致吞吐能力极差，因为很可能在刚好要`sleep`的时候，所有的`fd`都准备好IO数据，而这个时候却要硬生生的等待1s，可想而知...

那`for`循环里面就不`sleep`嘛，这样不就能及时处理了吗？

及时是及时了，但是不加`sleep`，那在没有`fd`需要处理的时候，估计CPU都要跑到100% 了。这个也是无法接受的。

纠结了，那`sleep`吞吐不行，不`sleep`浪费CPU，怎么办？

这种情况用户态很难有所作为，只能求助内核来提供机制协助来。因为内核才能及时的管理这些通知和调度。

## 这事Linux内核必须要给个说法？

我们再梳理下IO多路复用的需求和原理：IO多路复用就是1个线程处理多个fd的模式。

我们的要求是：这个“1“就要尽可能的快，避免一切无效工作，**要把所有的时间都用在处理句柄的IO上，不能有任何空转，sleep的时间浪费。**

Linux内核有没有一种工具，我们把一箩筐的`fd`放到里面，只要有一个`fd`能够读写数据，后台`loop`线程就要立马唤醒，全部马力跑起来。其他时间要把CPU让出去。

有，这种需求只能内核提供机制满足你，毕竟IO的处理都是内核之中，数据好没好内核最清楚。

内核提供了3种工具`select, poll, epoll`。为什么有3种？历史不断改进，`矬 -> 较矬 -> 卧槽、高效`的演变而已。

而这三种方式以`epoll`池的效率最高。为什么效率最高？

其实很简单，这里不详说，其实无非就是`epoll`做的无用功最少，`select`和`poll`或多或少都要多余的拷贝，盲猜（遍历才知道）`fd`，所以效率自然就低了。

举个例子，以`select`和`epoll`来对比举例，池子里管理了1024个句柄，`loop`线程被唤醒的时候，`select`都是蒙的，都不知道这1024个`fd`里谁IO准备好了。这种情况怎么办？只能遍历这1024个`fd`，一个个测试。假如只有一个句柄准备好了，那相当于做了1千多倍的无效功。

`epoll`则不同，从`epoll_wait`醒来的时候就能精确的拿到就绪的`fd`数组，不需要任何测试，拿到的就是要处理的。

## Epoll池原理

### Epoll涉及的系统调用

```shell
# man epoll
epoll_create # 负责创建一个池子，一个监控和管理句柄fd的池子；
epoll_ctl # 负责管理这个池子里的fd增、删、改；
epoll_wait # 就是负责打盹的，让出CPU调度，但是只要有“事”，立马会从这里唤醒；
```

### Epoll的ET模式与LT模式

LT（Level Trigger，水平模式）是`epoll`的默认操作模式，当`epoll_wait`函数检测到有事件发生并将通知应用程序，而应用程序不一定必须立即进行处理，这样`epoll_wait`函数再次检测到此事件的时候还会通知应用程序，直到事件被处理。

ET（Edge Trigger，边缘模式）模式，只要`epoll_wait`函数检测到事件发生，通知应用程序立即进行处理，后续的`epoll_wait`函数将不再检测此事件。因此ET模式在很大程度上降低了同一个事件被`epoll`触发的次数，因此效率比LT模式高。

> 为什么`epoll`默认是LT？
>
> LT是缺省的工作方式，并且同时支持`block`和`no-block socket`。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的`fd`进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的`select/poll`都是这种模型的代表。

### Epoll高效的原理

Linux下，`epoll`一直被吹爆，作为高并发IO实现的秘密武器。其中原理其实非常朴实：**epoll的实现几乎没有做任何无效功。** 我们从使用的角度切入来一步步分析下：

首先，`epoll`的第一步是创建一个池子。这个使用`epoll_create`来做：

```C
// 函数原型：int epoll_create(int size);
// 这个池子对我们来说是用来装fd的黑盒，我们暂不纠结其中细节，
// 我们拿到了一个epollfd，epollfd能唯一代表这个epoll池。
epollfd = epoll_create(1024);
if (epollfd == -1) {
    perror("epoll_create");
    exit(EXIT_FAILURE);
}
```

然后，我们就要往这个`epoll`池里放`fd`了，这就要用到`epoll_ctl`了：

```C
// 函数原型：int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 我们就把句柄11放到这个池子里，
// op（EPOLL_CTL_ADD）表明操作是增加、修改、删除，
// event 结构体可以指定监听事件类型，可读、可写。
if (epoll_ctl(epollfd, EPOLL_CTL_ADD, 11, &ev) == -1) {
    perror("epoll_ctl: listen_sock");
    exit(EXIT_FAILURE);
}
```

**第一个跟高效相关的问题来了，添加`fd`进池子也就算了，如果是修改、删除呢？怎么做到时间快？**

这里就涉及到你怎么管理`fd`的数据结构。最常见的思路：用`list`，可以吗？功能上可以，但是性能上拉垮。`list`的结构来管理元素，时间复杂度都太高`O(n)`，每次要一次次遍历链表才能找到位置。池子越大，性能会越慢。

那有简单高效的数据结构吗？

有，红黑树。Linux内核对于`epoll`池的内部实现就是用红黑树的结构体来管理这些注册进程来的句柄`fd`。红黑树是一种平衡二叉树，时间复杂度为`O(log n)`，就算这个池子就算不断的增删改，也能保持非常稳定的查找性能。

**现在思考第二个高效的秘密：怎么才能保证数据准备好之后，立马感知呢？**

`epoll_ctl`这里会涉及到一点。秘密就是：**回调的设置**。在`epoll_ctl`的内部实现中，除了把句柄结构用红黑树管理，另一个核心步骤就是设置`poll`回调。

**思考来了：`poll`回调是什么？怎么设置？**

先说说`file_operations->poll`是什么？

Linux设计成一切皆是文件的架构，实现一个文件系统的时候，就要实现这个文件调用，这个结构体用`struct file_operations`来表示。这个结构体有非常多的函数，精简一些如下：

```C
struct file_operations {
    ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
    ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
    __poll_t (*poll) (struct file *, struct poll_table_struct *);
    int (*open) (struct inode *, struct file *);
    int (*fsync) (struct file *, loff_t, loff_t, int datasync);
    // ....
};
```

你看到了`read`，`write`，`open`，`fsync`，`poll`等等，这些都是对文件的定制处理操作，对于文件的操作其实都是在这个框架内实现逻辑而已，比如ext2如果有对`read/write`做定制化，那么就会是`ext2_read`，`ext2_write`，ext4就会是`ext4_read`，`ext4_write`。在`open`具体“文件”的时候会赋值对应文件系统的`file_operations`给到file结构体。

`read`是文件系统定制`fd`读的行为调用，`write`是文件系统定制`fd`写的行为调用，`file_operations->poll`呢？

这个是定制监听事件的机制实现。通过`poll`机制让上层能直接告诉底层，我这个`fd`一旦读写就绪了，请底层硬件（比如网卡）回调的时候自动把这个`fd`相关的结构体放到指定队列中，并且唤醒操作系统。

举个例子：网卡收发包其实走的异步流程，操作系统把数据丢到一个指定地点，网卡不断的从这个指定地点掏数据处理。请求响应通过中断回调来处理，中断一般拆分成两部分：硬中断和软中断。`poll`函数就是把这个软中断回来的路上再加点料，只要读写事件触发的时候，就会立马通知到上层，采用这种事件通知的形式就能把浪费的时间窗就完全消失了。

**这个`poll`事件回调机制则是`epoll`池高效最核心原理。**

**`epoll`池管理的句柄只能是支持了`file_operations->poll`的文件`fd`。换句话说，如果一个“文件”所在的文件系统没有实现`poll`接口，那么就用不了`epoll`机制。**

在`epoll_ctl`下来的实现中，有一步是调用`vfs_poll`这个里面就会有个判断，如果`fd`所在的文件系统的`file_operations`实现了`poll`，那么就会直接调用，如果没有，那么就会报告响应的错误码。

```C
static inline __poll_t vfs_poll(struct file *file, struct poll_table_struct *pt)
{
    if (unlikely(!file->f_op->poll))
        return DEFAULT_POLLMASK;
    return file->f_op->poll(file, pt);
}
```

> Note：类似ext2，ext4，xfs这种常规的文件系统是没有实现的，换句话说，这些你最常见的、真的是文件的文件系统反倒是用不了`epoll`机制的。
>
> 那谁支持呢？
>
> socket fd，eventfd，timerfd这些实现了poll调用的可以放到`epoll`池进行管理。

**第二个问题：`poll`怎么设置？**

`poll`调用里面究竟是实现了什么？

总结概括来说：挂了个钩子，设置了唤醒的回调路径。`epoll`跟底层对接的回调函数是：`ep_poll_callback`，这个函数其实很简单，做两件事情：

1. 把事件就绪的`fd`对应的结构体放到一个特定的队列（就绪队列，ready list）；
2. 唤醒`epoll`，活来啦！

当`fd`满足可读可写的时候就会经过层层回调，最终调用到这个回调函数，把**对应`fd`的结构体**放入就绪队列中，从而把`epoll`从`epoll_wait`出唤醒。

这个对应结构体是什么？结构体叫做`epitem`，每个注册到`epoll`池的`fd`都会对应一个。

就绪队列很高级吗？

就绪队列因为没有查找的需求了呀，只要是在就绪队列中的`epitem`，都是事件就绪的，必须处理的。所以就绪队列就是一个最简单的双指针链表。

### 小结

**`epoll`之所以做到了高效，最关键的两点：**

- 内部管理`fd`使用了高效的红黑树结构管理，做到了增删改之后性能的优化和平衡；
- `epoll`池添加`fd`的时候，调用`file_operations->poll`，把这个`fd`就绪之后的回调路径安排好，通过事件通知的形式，做到最高效的运行。

**`epoll`池核心的两个数据结构：红黑树和就绪列表：**

- 红黑树是为了应对用户的增删改需求；
- 就绪列表是`fd`事件就绪之后放置的特殊地点，`epoll`池只需要遍历这个就绪链表，就能给用户返回所有已经就绪的`fd`数组。

## 总结

1. IO多路复用的原始实现很简单，就是一个1对多的服务模式，一个loop对应处理多个`fd`；
2. IO多路复用想要做到真正的高效，必须要内核机制提供。因为IO的处理和完成是在内核，如果内核不帮忙，用户态的程序根本无法精确的抓到处理时机；
3. `fd`记得要设置成非阻塞；
4. `epoll`池通过高效的内部管理结构，并且结合操作系统提供的`poll`事件注册机制，实现了高效的`fd`事件管理，为高并发的IO处理提供了前提条件；
5. `epoll`全名`eventpoll`，在Linux内核下以一个文件系统模块的形式实现，所以有人常说`epoll`其实本身就是文件系统也是对的；
6. socketfd，eventfd，timerfd这三种”文件“`fd`实现了`poll`接口，都可以使用`epoll_ctl`注册到池子里。我们最常见的就是网络的多路复用；
7. ext2，ext4，xfs这种真正意义的文件系统反倒没有提供`poll`接口实现，所以不能用`epoll`池来管理其句柄。那文件就无法使用 epoll 机制了吗？不是的，有一个库叫做`libaio`，通过这个库我们可以间接的让文件使用`epoll`通知事件。

